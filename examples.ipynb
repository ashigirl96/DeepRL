{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTHONPATH=\"$(pwd):$PYTHONPATH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_rl import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN\n",
    "def dqn_feature(**kwargs):\n",
    "    generate_tag(kwargs)\n",
    "    kwargs.setdefault('log_level', 0)\n",
    "    kwargs.setdefault('n_step', 1)\n",
    "    kwargs.setdefault('replay_cls', UniformReplay)\n",
    "    kwargs.setdefault('async_replay', True)\n",
    "    config = Config()\n",
    "    config.merge(kwargs)\n",
    "\n",
    "    config.task_fn = lambda: Task(config.game)\n",
    "    config.eval_env = config.task_fn()\n",
    "\n",
    "    config.optimizer_fn = lambda params: torch.optim.RMSprop(params, 0.001)\n",
    "    config.network_fn = lambda: VanillaNet(config.action_dim, FCBody(config.state_dim))\n",
    "    # config.network_fn = lambda: DuelingNet(config.action_dim, FCBody(config.state_dim))\n",
    "    config.history_length = 1\n",
    "    config.batch_size = 10\n",
    "    config.discount = 0.99\n",
    "    config.max_steps = 1e5\n",
    "\n",
    "    replay_kwargs = dict(\n",
    "        memory_size=int(1e4),\n",
    "        batch_size=config.batch_size,\n",
    "        n_step=config.n_step,\n",
    "        discount=config.discount,\n",
    "        history_length=config.history_length)\n",
    "\n",
    "    config.replay_fn = lambda: ReplayWrapper(config.replay_cls, replay_kwargs, config.async_replay)\n",
    "    config.replay_eps = 0.01\n",
    "    config.replay_alpha = 0.5\n",
    "    config.replay_beta = LinearSchedule(0.4, 1.0, config.max_steps)\n",
    "\n",
    "    config.random_action_prob = LinearSchedule(1.0, 0.1, 1e4)\n",
    "    config.target_network_update_freq = 200\n",
    "    config.exploration_steps = 1000\n",
    "    # config.double_q = True\n",
    "    config.double_q = False\n",
    "    config.sgd_update_frequency = 4\n",
    "    config.gradient_clip = 5\n",
    "    config.eval_interval = int(5e3)\n",
    "    config.async_actor = False\n",
    "    run_steps(DQNAgent(config))\n",
    "\n",
    "\n",
    "def dqn_pixel(**kwargs):\n",
    "    generate_tag(kwargs)\n",
    "    kwargs.setdefault('log_level', 0)\n",
    "    kwargs.setdefault('n_step', 1)\n",
    "    kwargs.setdefault('replay_cls', UniformReplay)\n",
    "    kwargs.setdefault('async_replay', True)\n",
    "    config = Config()\n",
    "    config.merge(kwargs)\n",
    "\n",
    "    config.task_fn = lambda: Task(config.game)\n",
    "    config.eval_env = config.task_fn()\n",
    "\n",
    "    config.optimizer_fn = lambda params: torch.optim.RMSprop(\n",
    "        params, lr=0.00025, alpha=0.95, eps=0.01, centered=True)\n",
    "    config.network_fn = lambda: VanillaNet(config.action_dim, NatureConvBody(in_channels=config.history_length))\n",
    "    # config.network_fn = lambda: DuelingNet(config.action_dim, NatureConvBody(in_channels=config.history_length))\n",
    "    config.random_action_prob = LinearSchedule(1.0, 0.01, 1e6)\n",
    "    config.batch_size = 32\n",
    "    config.discount = 0.99\n",
    "    config.history_length = 4\n",
    "    config.max_steps = int(2e7)\n",
    "    replay_kwargs = dict(\n",
    "        memory_size=int(1e6),\n",
    "        batch_size=config.batch_size,\n",
    "        n_step=config.n_step,\n",
    "        discount=config.discount,\n",
    "        history_length=config.history_length,\n",
    "    )\n",
    "    config.replay_fn = lambda: ReplayWrapper(config.replay_cls, replay_kwargs, config.async_replay)\n",
    "    config.replay_eps = 0.01\n",
    "    config.replay_alpha = 0.5\n",
    "    config.replay_beta = LinearSchedule(0.4, 1.0, config.max_steps)\n",
    "\n",
    "    config.state_normalizer = ImageNormalizer()\n",
    "    config.reward_normalizer = SignNormalizer()\n",
    "    config.target_network_update_freq = 10000\n",
    "    config.exploration_steps = 50000\n",
    "    # config.exploration_steps = 100\n",
    "    config.sgd_update_frequency = 4\n",
    "    config.gradient_clip = 5\n",
    "    config.double_q = False\n",
    "    config.async_actor = True\n",
    "    run_steps(DQNAgent(config))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    mkdir('log')\n",
    "    mkdir('tf_log')\n",
    "    set_one_thread()\n",
    "    random_seed()\n",
    "    # -1 is CPU, a positive integer is the index of GPU\n",
    "    select_device(-1)\n",
    "    game = 'BreakoutNoFrameskip-v4'\n",
    "    dqn_pixel(game=game, n_step=1, replay_cls=UniformReplay, async_replay=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
